{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "**Underfit**: the curve/hypothesis function cannot fit the training set well\n",
    "- Means the algorithm has **High bias** (pre-conception)\n",
    "\n",
    "**Overfit**: the learned hypothesis may fit the training set well, but fail to generalize to new examples \n",
    "- The generated hypothesis has **High variance**\n",
    "\n",
    "## Solutions\n",
    "- Reduce number of features\n",
    "    - Manually select or use selection algorithm\n",
    "    - At the cost of throwing away some useful information\n",
    "- Regularization: Keep all the features, but **reduce magnitude** of parameters $\\theta_j$\n",
    "    - Work well when we have lots of features, each of which contributes slightly (not largely) to predicting $y$\n",
    "    \n",
    "## Regularization\n",
    "Use **penalty (Cost)** to make parameters small: Add penalty to the cost function\n",
    "- Select a few target parameters $\\theta$ or apply to all parameters\n",
    "    - $min_\\theta J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta^2_j$\n",
    "    - Do not penalize the $\\theta_0$\n",
    "- Use Regularization Parameter $\\lambda$ to control the tradeoff between the underfitting and overfitting\n",
    "    - Extremely large $\\lambda$ will force all $\\theta$ to become 0, which results in underfitting\n",
    "    \n",
    "### Regularized Linear Regression\n",
    "#### Regularize the gradient descent\n",
    "Repeat{\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\alpha*\\frac{1}{m}\\sum_{i=1}^m[(h_{\\theta}(x^{(i)})-y^{(i)})*x_0^{(i)}]$\n",
    "    \n",
    "$\\theta_j := \\theta_j - \\alpha*\\{\\frac{1}{m}\\sum_{i=1}^m[(h_{\\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}]+\\frac{\\lambda}{m}\\theta_j\\}$=$\\theta_j(1-\\alpha*\\frac{\\lambda}{m}\\theta_j) - \\alpha*\\frac{1}{m}\\sum_{i=1}^m[(h_{\\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}]$$, j \\in {1, 2, ...,n}$\n",
    "\n",
    "}\n",
    "\n",
    "Observation: In addition to the regular reduction through gradient descent, the $\\theta_j$ also **shrinks with the factor** $(1-\\alpha*\\frac{\\lambda}{m}\\theta_j)$\n",
    "\n",
    "#### Regularize the Normal Equation\n",
    "![W3-REG-LP-NE](Plots/W3-REG-LP-NE.png)\n",
    "Observation: L is an identity matrix with size (n+1)*(n+1)\n",
    "\n",
    "### Regularized Logistic Regression\n",
    "$J(\\theta)=-\\frac{1}{m}\\sum^m_{i=1}[y^{(i)}log(h_{\\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))]+\\lambda \\sum_{j=1}^n \\theta^2_j$\n",
    "\n",
    "#### Regularize the gradient descent\n",
    "Repeat{\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\alpha*\\frac{1}{m}\\sum_{i=1}^m[(h_{\\theta}(x^{(i)})-y^{(i)})*x_0^{(i)}]$\n",
    "    \n",
    "$\\theta_j := \\theta_j - \\alpha*\\{\\frac{1}{m}\\sum_{i=1}^m[(h_{\\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}]+\\frac{\\lambda}{m}\\theta_j\\}, j \\in {1, 2, ...,n}$\n",
    "\n",
    "}\n",
    "\n",
    "## Visualize with the Learning Curve\n",
    "### During the High Bias\n",
    "![W6-LEARNING-CURVE-HB](Plots/W6-LEARNING-CURVE.png)\n",
    "- Both the training error and test error are higher than the desired performance\n",
    "- The gap is decreasing\n",
    "- The training error will **increase** with the sample size\n",
    "\n",
    "### During the High Variance\n",
    "![W6-LEARNING-CURVE-HV](Plots/W6-LEARNING-CURVE2.png)\n",
    "- The gap is decreasing\n",
    "- The training error will **increase** with the sample size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
