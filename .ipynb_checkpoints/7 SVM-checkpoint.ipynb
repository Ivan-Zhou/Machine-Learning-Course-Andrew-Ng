{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Hypothesis\n",
    "$min_{\\theta}\\ h_{\\theta}(x) = C*\\sum_{i=1}^m[y^{(i)}*cost_1(\\theta^T x^{(i)})+(1-y^{(i)})*cost_0(\\theta^T x^{(i)})]+\\frac{1}{2}\\sum^n_{i=1}\\theta^2_j$\n",
    "- $Cost$ function in the equation above: a piecewise function that mimic the shape of the sigmoid function\n",
    "    - If y = 1, we want $Z = \\theta^T x^{(i)} \\gg 0$\n",
    "    - 0: otherwise\n",
    "    - In the image below: the intercept on the z-axis is not necessary at $z=1$\n",
    "![W7-SVM-Cost](Plots/W7-SVM-Cost.png)\n",
    "- $C$: the coefficient that replaces $\\lambda$; when $C$ is large, the clussifier will be very sensitive to the outliers, which means the presence of an outlier can impact the position of the original hyperplane. \n",
    "    - $C$ can be thought as $\\frac{1}{\\lambda}$ in the regulated Logistic Regression model\n",
    "    - When $C$ is large, or $lambda$ is small, the model tends to have high variance (overfitting)\n",
    "    - Wehn $C$ is small, or $lambda$ is large, the model tends to have high bias (underfitting)\n",
    "    \n",
    "### Decision Boundary - Large Margin Classifier\n",
    "SVM tries to **maximize the margin** (minimum distance) between the *hyperplane* vs the *positive and negative examples*\n",
    "\n",
    "#### Optimization Function\n",
    "![W7-SVM-OPTIM](Plots/W7-SVM-OPTIM.png)\n",
    "- Come from the hypothesis function\n",
    "- We assume the costs (in the first half of the hypothesis function) will become zero that at the optimal boundary\n",
    "    - Thus omitted from the objective function\n",
    "    - Thus we have the two constraints\n",
    "- Our goals become to minimize the square product of $\\theta$s (through the objective function) while maintaining the classification functionality (through constraints)\n",
    "\n",
    "#### Basic Math - Vector Inner Product\n",
    "![W7-SVM-VECTORPRODUCT](Plots/W7-SVM-VECTORPRODUCT.png)\n",
    "- $u$ and $v$ are two vertical vectors\n",
    "- $||u||$ = length of the vector $u$: for example, $||u|| = \\sqrt{u^2_1+u^2_2} \\in R$\n",
    "- $p$ = length of the **projection** of $v$ onto $u$: $p = ||v||*cos(\\alpha)$\n",
    "- $u^T*v = ||u||*||v||*cos(\\alpha) = p*||u|| = \\sum_{i=1}^n u_i*v_i$\n",
    "\n",
    "#### Transform the Optimization Function w.r.t. $||\\theta||$\n",
    "![W7-SVM-OPTIM2](Plots/W7-SVM-OPTIM2.png)\n",
    "In the objective function: \n",
    "- $\\sum_{j=1}^n \\theta^2_j = (\\sqrt{\\sum_{j=1}^n \\theta^2_j})^2 = (\\sqrt{x_1^2+x_2^2+...})^2=||\\theta||^2$\n",
    "\n",
    "Constraints:\n",
    "- $\\theta^T x^{(i)} = p^{(i)}*||\\theta||$\n",
    "- Refer to the Vector Inner Product above\n",
    "- $p^{(i)}$ is the projection of $x^{(i)}$ onto the vector $\\theta$\n",
    "\n",
    "Meaning:\n",
    "- **We need to find a large $p^{(i)}$ in order to minimize $||\\theta||^2$**\n",
    "\n",
    "#### Find the Optimal Decision Boundary with Math\n",
    "![W7-SVM-BOUNDARY](Plots/W7-SVM-BOUNDARY.png)\n",
    "*Left is non-optimal, and Right is Optimal*\n",
    "\n",
    "The optimization tries to maximize the margin (minimum distance between the Boundary vs. Positive & Negative Data)\n",
    "- $\\theta$s are perpendicular to the decision boundary\n",
    "- $p^{(i)}$ is the projection of $x^{(i)}$ onto the vector $\\theta$\n",
    "- The objective function tries to find **as large $p$ as possible** in order to minimize $||\\theta||$\n",
    "    - Ruled by constraint, $p^{(i)}*||\\theta|| \\geq 1$ if $y^{(i)} = 1$ (or the constraint on the Negative data)\n",
    "    \n",
    "## SVM with Kernels\n",
    "**Enable SVM to work on non-linear boundaries**\n",
    "\n",
    "### Similarity Function \n",
    "Kernels are similarity functions describing the **proximity/similarity between a data point $x$ and one landmark $l^{(i)}$**\n",
    "\n",
    "### Gaussian Kernel\n",
    "$f_i = similarity(x,l^{(i)}) = exp(-\\frac{||x-l^{(i)}||^2}{2*\\sigma^2})$\n",
    "- Numerator: the euclidean distance between $x$ and a landmark $l^{(i)}$\n",
    "- Denominator: variance coefficient\n",
    "\n",
    "#### Kernels and Classification\n",
    "- If $x$ is closed to $l^{(i)}$ or $x\\approx l^{(i)}$, $-\\frac{||x-l^{(i)}||^2}{2*\\sigma^2}\\approx 0$, so $f_i\\approx 1$\n",
    "- If $x$ is far from $l^{(i)}$, $-\\frac{||x-l^{(i)}||^2}{2*\\sigma^2}\\approx -\\infty$, so $f_i\\approx 0$\n",
    "- Therefore, we can get a value between 0 and 1 to  indicate **whether the data point $x$ is closed to certain landmarks**. \n",
    "- With multiple landmarks and the Kernel functions, we can **plot a non-linear boundary**\n",
    "\n",
    "![W7-KERNEL-BOUNDARY](Plots/W7-KERNEL-BOUNDARY.png)\n",
    "*Based on the inequation on the right, we can classify each data point and generate a non-linear boundary*\n",
    "\n",
    "### SVM with Kernels\n",
    "We replace the $x$ in the original SVM with kernels $f^{(i)}$\n",
    "$min_{\\theta}\\ h_{\\theta}(x) = C*\\sum_{i=1}^m[y^{(i)}*cost_1(\\theta^T f^{(i)})+(1-y^{(i)})*cost_0(\\theta^T f^{(i)})]+\\frac{1}{2}\\sum^n_{i=1}\\theta^2_j$\n",
    "\n",
    "#### Choose landmark $l^{(i)}$\n",
    "We use each $x$ in the data points in the training set as our landmarks\n",
    "- With $m$ training examples, we have $m$ landmarks\n",
    "- Implementation tips: $\\theta^2 = \\theta^T*\\theta$\n",
    "\n",
    "#### Choose Variance Coefficient $\\sigma$\n",
    "##### Visualize\n",
    "![W7-SIGMA-VIZ](Plots/W7-SIGMA-VIZ.png)\n",
    "\n",
    "## Implementation Notes\n",
    "#### Use existing SVM software packages (liblinear, libsvm, etc.) to solve for parameters $\\theta$\n",
    "\n",
    "#### Choice of kernels\n",
    "- Linear Kernels: approx. linear classifier\n",
    "    - Useful to prevent overfitting when you have large number of features ($n$) but small number of data points ($m$)\n",
    "    - Very similar to Logistic Regression in terms of the performance\n",
    "- Gaussian Kernels\n",
    "    - Useful when you have small number of features but large number of data, so you can fit a more complex decision boundaries\n",
    "    - Need to speicify the variance coeff $\\sigma^2$\n",
    "- Others: polynomial kernel, String Kernel, Chi-Square Kernel, histogram intersection kernel, etc.\n",
    "- Not all similarity functions are always valid\n",
    "    - Need to satisfy *Mercer's Therem* to ensure the optimality\n",
    "- Choose the kernel that performs the best on the cross-validation data\n",
    "\n",
    "#### Feature Scaling on all features before using the Gaussian kernal\n",
    "- The calculation of kernel includes the euclidean distance between $x$ and the landmark\n",
    "- Large feature may dominate the size of the distance\n",
    "\n",
    "#### Multi-class Classification\n",
    "- Many SVM packages already have built-in multi-class functionality\n",
    "- Otherwise, use one-vs-all methods\n",
    "\n",
    "#### Logistic Regression vs. SVM\n",
    "- If number of features $n$ is large (relative to the size of data), use logistic regression, or linear Kernel\n",
    "- If $n$ is small (1-1K) and $m$ is intermediate (10-10K), use SVM with Gaussian Kernel\n",
    "- If $n$ is small (1-1K) and $m$ is large (50K+), add more features, then use logistic regression, or linear Kernel\n",
    "- Neural Network is likely to work well for most of the settings, but may be slower to train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
