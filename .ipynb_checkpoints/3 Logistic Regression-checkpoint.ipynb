{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Logistic Regression\n",
    "### Hypothesis Representation\n",
    "$h_{\\theta}(x) = P(y=1|x;\\theta)$: the probability that the output is 1, given input x, parameterized by $\\theta$\n",
    "- $h_\\theta(x) \\geq 0.5, P = 1$\n",
    "- $h_\\theta(x) < 0.5, P = 0$\n",
    "\n",
    "### Logistic Function/Sigmoid Function\n",
    "$h_{\\theta}(x) = \\frac{1}{1+e^{-\\theta^Tx}}$\n",
    "- Derived from the Sigmoid Function\n",
    "    - $h_{\\theta}(x) = g(z)$\n",
    "    - Sigmoid Function: $g(z) = \\frac{1}{1+e^{-z}}$\n",
    "    - $z = \\theta^T X$\n",
    "![W3-Sigmoid](Plots/W3-Sigmoid.png)\n",
    "- Transforming an arbitrary-valued function into a function better suited for classification\n",
    "\n",
    "### Benefit\n",
    "- $h_{\\theta}(x)$ can always fall into the range of [0,1]\n",
    "\n",
    "### Not tot use Linear Regression\n",
    "- $h_{\\theta}(x)$ from the Linear Regression can be out of the target range (ie, [0,1])\n",
    "    - Even if the $y$s in the training set all fall into the range, the $h$ from the testing set may be out of the range\n",
    "- Classification may not be a linear function\n",
    "    - A line may not work effectively in classification\n",
    "\n",
    "### Decision Boundary\n",
    "$h_{\\theta}(x) = g(z) \\geq 0.5$, when $z = \\theta^T X \\geq 0$\n",
    "- Solve the equation $\\theta^T X = 0$ can provide us the decision boundary\n",
    "![W3-Decision-Boundary](Plots/W3-Decision-Boundary.png)\n",
    "\n",
    "The **Decision Boundary** is the line that **separate the area** where y = 0 and where y= 1\n",
    "- Decision boundary is a property of parameters $\\theta$s, not the data\n",
    "    - We can plot the decision boundary without the dataset\n",
    "\n",
    "### Cost Function\n",
    "$J(\\theta) = \\frac{1}{m} \\sum_{i = 1}^{m}Cost(h_{\\theta}(x^{(i)},y^{(i)})$\n",
    "- If y = 1, $Cost(h_{\\theta}(x^{(i)},y^{(i)}) = -log(h_{\\theta(x)})$\n",
    "    - The cost will be very high if $h_{\\theta(x)}$ approaches 0\n",
    "- If y = 0, $Cost(h_{\\theta}(x^{(i)},y^{(i)}) = -log(1-h_{\\theta(x)})$\n",
    "    - The cost will be very high if $h_{\\theta(x)}$ approaches 1\n",
    "![W3-LGR-COST](Plots/W3-LGR-COST.png)\n",
    "- Guarantee that $J(\\theta)$ is a convex\n",
    "    - If we use the same cost function as the Linear Regression, it will become non-convex, which is difficult to converge to the global minimum\n",
    "\n",
    "#### Simplified (Compressed) Cost Function\n",
    "- $Cost(h_{\\theta}(x),y)=-y log(h_{\\theta}(x))-(1-y)log(1-h_{\\theta}(x))$ \n",
    "- $J(\\theta)=-\\frac{1}{m}\\sum^m_{i=1}[y^{(i)}log(h_{\\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))]$\n",
    "- Take Derivative of $J(\\theta)$: $\\frac{\\partial}{\\partial \\theta_j}J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^m[(h_{\\theta}(x^{(i)})-y^{(i)}).*x^{(i)}]$\n",
    "    - [Good Reference](https://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression)\n",
    "\n",
    "#### Gradient Descent\n",
    "$\\theta_j := \\theta_j - \\alpha*\\frac{1}{m}\\sum_{i=1}^m[(h_{\\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}]$\n",
    "- Vectorized Implementation: $\\theta := \\theta - \\alpha*\\frac{1}{m}\\sum_{i=1}^m[(h_{\\theta}(x^{(i)})-y^{(i)}).*x^{(i)}]$\n",
    "\n",
    "Feature Scaling can also help speed up the Gradient Descent in Logistic Regression\n",
    "\n",
    "#### Other Optimatization Algorithms\n",
    "- Other than the Gradient Descent:\n",
    "    - Conjugate Gradient\n",
    "    - BFGS\n",
    "    - L-BFGS\n",
    "- Advantages:\n",
    "    - No need to define the learning rate\n",
    "    - Much faster than the Gradient Descent\n",
    "- Disadvantages\n",
    "    - More complex: require expertise to implement\n",
    "    \n",
    "### Multi-class Classification\n",
    "Example: Email Foldering, Tagging\n",
    "\n",
    "#### One-vs-All Method\n",
    "- Training: Train a Logistic Regression classifier $h^{(i)}_{\\theta}(x)$ for each class $i$ to predict the probability that $y=i$ ($y$ is in the class $i$)\n",
    "- Prediction: on a new input $x$, $i = argmax_{i}[h^{(i)}_{\\theta}(x)]$\n",
    "![W3-MULTI-CLASS](Plots/W3-MULTI-CLASS.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
