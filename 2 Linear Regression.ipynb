{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with One Variable (Univariate)\n",
    "### Model Representation\n",
    "- Notation\n",
    "\t- $m$ = Number of training examples\n",
    "\t- $x$ = input variable (feature)\n",
    "\t- $y$ = output variable (target)\n",
    "\t- $(x, y)$ = one training example\n",
    "\t- $(x^i, y^i)$ = $i^{th}$ training example \n",
    "\t- $h$ = hypothesis\n",
    "\t- $\\theta$ = Parameters\n",
    "- Use the training set and the learning algorithm to learn a hypothesis function $h(x)$\n",
    "    - We use the hypothesis to map x's to y's $h: x -> y$\n",
    "    - Linear Example: $h_{\\theta}(x) = \\theta_0+\\theta_1 x$\n",
    "\n",
    "### Cost Function\n",
    "- Hypothesis Function: $h_θ(x) = θ_0 + θ_1*x$\n",
    "- Cost Function: $J(\\theta_0,\\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m (\\hat{y_i} - y_i)^2=\\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x_i)-y_i)^2$\n",
    "    - Measure the (vertical) distance between the predicted value and the actual value\n",
    "    - $\\frac{1}{2}$ is used as a convenience for the computation of the graident descent\n",
    "- **Goal**: Minimize $J(\\theta_0,\\theta_1)$\n",
    "    - Choose $θ_0$, $θ_1$ so that $h_θ(x)$ is close to y for the training examples\n",
    "    - Find the best fit straight line for the data\n",
    "    \n",
    "#### Visualization of Cost Function in a 3D Plot\n",
    "![3D-Cost-Function](Plots/W1-CF-3D.png)\n",
    "\n",
    "#### Visualization of Cost Function in a Contour Plot\n",
    "A contour plot is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line. \n",
    "![Contour-Cost-Function](Plots/W1-CF-CTP.png)\n",
    "\n",
    "> Minimize the cost function $J(\\theta_0,\\theta_1)$ with the Gradient Descent\n",
    "\n",
    "### Gradient Descent Algorithm\n",
    "repeat until convergence{\n",
    "\n",
    "  $\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta_0,\\theta_1), \\forall j \\in J$\n",
    "  \n",
    "  or $\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}, \\forall j \\in J$\n",
    "}\n",
    "\n",
    "- **Simultaneous update**: all $\\theta$'s are updated with the same pair of the $\\theta$ values on the rhs\n",
    "    - Do not use the updated $\\theta$ to compute others until all $\\theta$'s have been updated\n",
    "- Learning Rate $\\alpha$: \n",
    "    - If $\\alpha$ is too small, gradient descent can be slow\n",
    "    - If $\\alpha$ is too large, gradient descent can overshoot the minimum or even diverge\n",
    "\n",
    "#### Gradient Descent in Linear Regression\n",
    "All linear models are **convex function** - they only have one unique local minimum\n",
    "    - Gradient Descent is guaranteed to converge to the global minimum\n",
    "    - Bowl-shaped function\n",
    "    \n",
    "![Gradient Descent in Linear Regression](Plots/W1-GD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "Hypothesis: $h_{\\theta}(x) = \\theta_0 x_0 + \\theta_1 x_1 +...+ \\theta_n x_n = \\theta^T x$\n",
    "- $\\theta_0$ is defined as 0 and created for the convenience of notation\n",
    "- $x\\in R^{n+1}$, $\\theta \\in R^{n+1}$, both of them are 1x(n+1) vector\n",
    "- $n$: number of features\n",
    "\n",
    "### Gradient Descent for Multivariate Linear Regression\n",
    "repeat until convergence{\n",
    "\n",
    "  $\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta_0,\\theta_1), \\forall j \\in J$\n",
    "  \n",
    "  or $\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}, \\forall j \\in J$\n",
    "\n",
    "}\n",
    "\n",
    "#### Feature Scaling\n",
    "Goal: Make features on a similar scale (approximately in the range of [-1,1])\n",
    "- From contour perspective: the shape of the eclipses will be closer to a round circle\n",
    "- **Will save time of the Gradient Descent** (the gradient will not be too large/small\n",
    "\n",
    "#### Mean Normalization\n",
    "Have sample mean approximately to 0\n",
    "$x_i = \\frac{x_i - \\mu_i}{s_i}$\n",
    "- $\\mu_i$: the sample average\n",
    "- $s_i$: the sample range\n",
    "\n",
    "#### Choose Learning Rate\n",
    "- The cost $J(\\theta)$ should ** continuously decrease** with the Number of iterations\n",
    "    - For sufficiently small $\\alpha$, J should decrease with every iteration\n",
    "- Fast decrease rate is desired\n",
    "- Plot of Iteration Numbers vs. $J(\\theta)$ is helpful with debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "Use quadratic variables, cubic variables, or multiple variables\n",
    "\n",
    "Polynomial Regression can be convert to Multivariate Linear Regression by replacing high power variable with a new variable\n",
    "- **Feature scaling becomes important**\n",
    "\n",
    "#### Example\n",
    "$h_{\\theta}(x) = \\theta_0 + \\theta_1(size)+\\theta_2 (size^2)$\n",
    "\n",
    "Convert to: $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1+\\theta_2 x_2$, where $x_2 = x_1^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation\n",
    "Solve for $\\theta$ analytically\n",
    "- Advantages\n",
    "    - Get to the optimality in just **one iteration**\n",
    "    - No need to choose a learning rate\n",
    "- Disadvantage\n",
    "    - Slow if the number of features ($n$) is very large: need to compute $(X^TX)^{-1}$ \n",
    "        - In comparison: the complexity of Normal Equation is $O(n^3)$, while the complexity of Gradient Descent is $O(kn^2)$\n",
    "        - Rule of thumb: n < 10,000 is fine\n",
    "\n",
    "In addition: Feature Scaling is not necessary\n",
    "\n",
    "### Method\n",
    "Take partial derivative with respect to $\\theta_i$ and solve for $\\theta_i$ \n",
    "- Formula: Solve for $\\theta = (X^TX)^{-1} X^T y$\n",
    "    - The X here is a Design Matrix ($m*(n+1)$)\n",
    "        - Each row is a sample ($m$)\n",
    "        - Each column is a feature (including the first column of 1s for $x_0$)\n",
    "    - The dimension of $\\theta$ is (n+1,1)\n",
    "![W2-Normal-EQ](Plots/W2-Normal-EQ.png)\n",
    "- Octave Code: `pinv(X'*X)*X'*y`\n",
    "    - Here we use Psedo-inverse, which can return a approximate inverse value even if the matrix is not invertible\n",
    "\n",
    "### Non-invertible matrix (Singular/Degenerate Matrix)\n",
    "Problem: if $X^TX$ is not invertible, then we cannot compute $\\theta = (X^TX)^{-1} X^T y$ and find the optimal value\n",
    "\n",
    "#### Causes\n",
    "- There are redundant features: features that are linearly dependent\n",
    "    - One column can be written as a linear function of the other columns - become trivial when solving equations ([Explanation](https://stats.stackexchange.com/questions/152663/reasons-for-a-non-invertible-matrix))\n",
    "    - Example: $x_1 = 2x_2$\n",
    "    - **Solution**: Delete one of the redundant features\n",
    "- There are too many features ($m \\leq n$)\n",
    "    - **Solution**: Delete some features, or use regularization\n",
    "    \n",
    "#### Solutions in implementation\n",
    "- Use 'pinv()' in Octave or 'ginv()' in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(list,target):\n",
    "    return (target-sum(list)/len(list))/(max(list)-min(list))\n",
    "\n",
    "normalize([89,72,94,69],89)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
