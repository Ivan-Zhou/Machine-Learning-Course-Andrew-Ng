{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with One Variable (Univariate)\n",
    "### Model Representation\n",
    "- Notation\n",
    "\t- $m$ = Number of training examples\n",
    "\t- $x$ = input variable (feature)\n",
    "\t- $y$ = output variable (target)\n",
    "\t- $(x, y)$ = one training example\n",
    "\t- $(x^i, y^i)$ = $i^{th}$ training example \n",
    "\t- $h$ = hypothesis\n",
    "\t- $\\theta$ = Parameters\n",
    "- Use the training set and the learning algorithm to learn a hypothesis function $h(x)$\n",
    "    - We use the hypothesis to map x's to y's $h: x -> y$\n",
    "    - Linear Example: $h_{\\theta}(x) = \\theta_0+\\theta_1 x$\n",
    "\n",
    "### Cost Function\n",
    "- Hypothesis Function: $h_θ(x) = θ_0 + θ_1*x$\n",
    "- Cost Function: $J(\\theta_0,\\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m (\\hat{y_i} - y_i)^2=\\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x_i)-y_i)^2$\n",
    "    - Measure the (vertical) distance between the predicted value and the actual value\n",
    "    - $\\frac{1}{2}$ is used as a convenience for the computation of the graident descent\n",
    "- **Goal**: Minimize $J(\\theta_0,\\theta_1)$\n",
    "    - Choose $θ_0$, $θ_1$ so that $h_θ(x)$ is close to y for the training examples\n",
    "    - Find the best fit straight line for the data\n",
    "    \n",
    "#### Visualization of Cost Function in a 3D Plot\n",
    "![3D-Cost-Function](Plots/W1-CF-3D.png)\n",
    "\n",
    "#### Visualization of Cost Function in a Contour Plot\n",
    "A contour plot is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line. \n",
    "![Contour-Cost-Function](Plots/W1-CF-CTP.png)\n",
    "\n",
    "> Minimize the cost function $J(\\theta_0,\\theta_1)$ with the Gradient Descent\n",
    "\n",
    "### Gradient Descent Algorithm\n",
    "repeat until convergence{\n",
    "\n",
    "  $\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta_0,\\theta_1), \\forall j \\in J$\n",
    "  \n",
    "  or $\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}, \\forall j \\in J$\n",
    "}\n",
    "\n",
    "- **Simultaneous update**: all $\\theta$'s are updated with the same pair of the $\\theta$ values on the rhs\n",
    "    - Do not use the updated $\\theta$ to compute others until all $\\theta$'s have been updated\n",
    "- Learning Rate $\\alpha$: \n",
    "    - If $\\alpha$ is too small, gradient descent can be slow\n",
    "    - If $\\alpha$ is too large, gradient descent can overshoot the minimum or even diverge\n",
    "\n",
    "#### Gradient Descent in Linear Regression\n",
    "All linear models are **convex function** - they only have one unique local minimum\n",
    "    - Gradient Descent is guaranteed to converge to the global minimum\n",
    "    - Bowl-shaped function\n",
    "    \n",
    "![Gradient Descent in Linear Regression](Plots/W1-GD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "Hypothesis: $h_{\\theta}(x) = \\theta_0 x_0 + \\theta_1 x_1 +...+ \\theta_n x_n = \\theta^T x$\n",
    "- $\\theta_0$ is defined as 0 and created for the convenience of notation\n",
    "- $x\\in R^{n+1}$, $\\theta \\in R^{n+1}$, both of them are 1x(n+1) vector\n",
    "- $n$: number of features\n",
    "\n",
    "### Gradient Descent for Multivariate Linear Regression\n",
    "repeat until convergence{\n",
    "\n",
    "  $\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta_0,\\theta_1), \\forall j \\in J$\n",
    "  \n",
    "  or $\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})*x_j^{(i)}, \\forall j \\in J$\n",
    "\n",
    "}\n",
    "\n",
    "#### Feature Scaling\n",
    "Goal: Make features on a similar scale (approximately in the range of [-1,1])\n",
    "- From contour perspective: the shape of the eclipses will be closer to a round circle\n",
    "- **Will save time of the Gradient Descent** (the gradient will not be too large/small\n",
    "\n",
    "#### Mean Normalization\n",
    "Have sample mean approximately to 0\n",
    "$x_i = \\frac{x_i - \\mu_i}{s_i}$\n",
    "- $\\mu_i$: the sample average\n",
    "- $s_i$: the sample range\n",
    "\n",
    "#### Choose Learning Rate\n",
    "- The cost $J(\\theta)$ should ** continuously decrease** with the Number of iterations\n",
    "    - For sufficiently small $\\alpha$, J should decrease with every iteration\n",
    "- Fast decrease rate is desired\n",
    "- Plot of Iteration Numbers vs. $J(\\theta)$ is helpful with debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "Use quadratic variables, cubic variables, or multiple variables\n",
    "\n",
    "Polynomial Regression can be convert to Multivariate Linear Regression by replacing high power variable with a new variable\n",
    "- **Feature scaling becomes important**\n",
    "\n",
    "#### Example\n",
    "$h_{\\theta}(x) = \\theta_0 + \\theta_1(size)+\\theta_2 (size^2)$\n",
    "\n",
    "Convert to: $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1+\\theta_2 x_2$, where $x_2 = x_1^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
