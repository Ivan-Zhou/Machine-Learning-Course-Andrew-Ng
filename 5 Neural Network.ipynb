{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Representation\n",
    "### Components of Neural Network\n",
    "#### A Single Logistic Neuron Unit\n",
    "- Input features\n",
    "    - The $x_0$ input is used as \"bias unit\" and always equal to 1\n",
    "- Input Wire\n",
    "- Neuron\n",
    "- Output Wire\n",
    "- Output Hypothesis $h_{\\theta}(x) = \\frac{1}{1+e^{-\\theta^Tx}}$\n",
    "\n",
    "![W4-LOGISTIC-UNIT](Plots/W4-LOGISTIC-UNIT.png)\n",
    "\n",
    "#### Activation Units\n",
    "Nodes in the intermediate/hidden layers\n",
    "- $a^{(j)}_i$: Activation of unit $i$ in layer $j$ (be aware of the notation)\n",
    "- $\\Theta^{(j)}_{b,a}$: matrix of weights controlling function mapping from the node $a$ in layer $j$ to the node $b$ in the layer $j+1$\n",
    "    - Dimension of $\\Theta^{(j)}$: $s_{j+1}*(S_{j}+1)$ if the layer $j$ has $s_j$ units and the layer $j+1$ has $s_{j+1}$ units\n",
    "    - Each layer has an **additional bias unit** $x_0$/$a_0$\n",
    "\n",
    "### Forward Propagation - Compute the Hypothesis\n",
    "In the Neural Net below, we have 3 nodes in the input layers and 3 nodes in the second layer (hidden layer), so the DImension of $\\Theta^{(1)}$ is $3*4 = 12$. \n",
    "\n",
    "The computation of the second layer (hidden layer) is shown with $a^{(2)}_1,a^{(2)}_2,a^{(2)}_3$.\n",
    "\n",
    "The final output $h_{\\Theta}$ is computed with $a^{(3)}_1$\n",
    "\n",
    "![W4-FORWARD-PROP](Plots/W4-FORWARD-PROP.png)\n",
    "![W4-NN-FP](Plots/W4-NN-FP.png)\n",
    "- On the right size, it shows the vectorized computation. Note: \n",
    "    - Remember to add bias unit (1) to each layer\n",
    "    - Use $z^{(j)}_k$ to represent the product inside the $g()$ function\n",
    "        - $z^{(j)} = \\Theta^{(j-1)}a^{(j-1)}$\n",
    "    - The $g()$ in the screenshot stands for the sigmoid function $g(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "### Examples - From AND/OR to XNOR Operation\n",
    "XNOR: gives 1 when $x_1=x_2$\n",
    "\n",
    "An important reference is the curve of the Sigmoid Function\n",
    "![W4-SIGMOID](Plots/W4-SIGMOID.png)\n",
    "![W4-NN-OPT](Plots/W4-NN-OPT.png)\n",
    "\n",
    "### Multiclass Classification\n",
    "Same as the traditional Logistic Regression, we use **One vs All** method\n",
    "- The hypothesis output $h_{\\theta}(x)$ is a vector instead of a number, and the class with the largest probability will be marked with 1\n",
    "\n",
    "![W4-NN-MM](Plots/W4-NN-MM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Backpropagation\n",
    "#### Cost Function\n",
    "#### Notation\n",
    "- $L$: the total number of layers in the network\n",
    "- $s_l$: the number of units (excluding the bias unit) in layer $l$\n",
    "- $K$: the number of output units/classes\n",
    "\n",
    "##### Cost of Weight Matrix\n",
    "![W5-COST](Plots/W5-COST.png)\n",
    "- Extra summation than the Regularized Logistic Regression to account for the multiple output nodes\n",
    "    - Loop through the number of output nodes (For example, 1 to 10)\n",
    "- $\\Theta_{j,i}^{(l)}$: the weight matrix that maps from the node $i$ in the layer $l$ to the node $j$ in the next layer \n",
    "\n",
    "#### Backpropagation Algorithm - Minimize Cost Function $J(\\Theta)$\n",
    "[Reference - Backpropagation in CS231](http://cs231n.github.io/optimization-2/)\n",
    "![W4-NN-BP](Plots/W4-NN-BP.png)\n",
    "\n",
    "- **Accumulate Losses**: Perform forward propagation to find $a^{(L)}$ (or $h_{\\theta}(x)$) of one $x^{(i)}$, and compute the loss with the corresponding $y^{(i)}$\n",
    "    - One pair at a time\n",
    "- After find the loss of the output layer $\\delta^{(L)}$, we can compute the $\\delta^{(l)}$ of the hidden layers through back-propagation\n",
    "![W5-DELTA](Plots/W5-DELTA.png)\n",
    "    - The second part of the formula comes from the g-prime\n",
    "    ![W5-GPRIME](Plots/W5-GPRIME.png)\n",
    "    - [Proof with the derivative of Sigmoid](http://mathworld.wolfram.com/SigmoidFunction.html): $\\frac{d}{dx} (\\frac{1}{1+e^{-x}}) = \\frac{e^{-x}}{(1+e^{-x})^2} = y(x)*[(1+y(x))]$\n",
    "\n",
    "- Vectorize the accumulation of $\\Delta$\n",
    "![W5-DELTA2](Plots/W5-DELTA2.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
