{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Representation\n",
    "### Components of Neural Network\n",
    "#### A Single Logistic Neuron Unit\n",
    "- Input features\n",
    "    - The $x_0$ input is used as \"bias unit\" and always equal to 1\n",
    "- Input Wire\n",
    "- Neuron\n",
    "- Output Wire\n",
    "- Output Hypothesis $h_{\\theta}(x) = \\frac{1}{1+e^{-\\theta^Tx}}$\n",
    "\n",
    "![W4-LOGISTIC-UNIT](Plots/W4-LOGISTIC-UNIT.png)\n",
    "\n",
    "#### Activation Units\n",
    "Nodes in the intermediate/hidden layers\n",
    "- $a^{(j)}_i$: Activation of unit $i$ in layer $j$ (be aware of the notation)\n",
    "- $\\Theta^{(j)}_{b,a}$: matrix of weights controlling function mapping from the node $a$ in layer $j$ to the node $b$ in the layer $j+1$\n",
    "    - Dimension of $\\Theta^{(j)}$: $s_{j+1}*(S_{j}+1)$ if the layer $j$ has $s_j$ units and the layer $j+1$ has $s_{j+1}$ units\n",
    "    - Each layer has an **additional bias unit** $x_0$/$a_0$\n",
    "\n",
    "### Forward Propagation - Compute the Hypothesis\n",
    "In the Neural Net below, we have 3 nodes in the input layers and 3 nodes in the second layer (hidden layer), so the DImension of $\\Theta^{(1)}$ is $3*4 = 12$. \n",
    "\n",
    "The computation of the second layer (hidden layer) is shown with $a^{(2)}_1,a^{(2)}_2,a^{(2)}_3$.\n",
    "\n",
    "The final output $h_{\\Theta}$ is computed with $a^{(3)}_1$\n",
    "\n",
    "![W4-FORWARD-PROP](Plots/W4-FORWARD-PROP.png)\n",
    "![W4-NN-FP](Plots/W4-NN-FP.png)\n",
    "- On the right size, it shows the vectorized computation. Note: \n",
    "    - Remember to add bias unit (1) to each layer\n",
    "    - Use $z^{(j)}_k$ to represent the product inside the $g()$ function\n",
    "        - $z^{(j)} = \\Theta^{(j-1)}a^{(j-1)}$\n",
    "    - The $g()$ in the screenshot stands for the sigmoid function $g(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "### Examples - From AND/OR to XNOR Operation\n",
    "XNOR: gives 1 when $x_1=x_2$\n",
    "\n",
    "An important reference is the curve of the Sigmoid Function\n",
    "![W4-SIGMOID](Plots/W4-SIGMOID.png)\n",
    "![W4-NN-OPT](Plots/W4-NN-OPT.png)\n",
    "\n",
    "### Multiclass Classification\n",
    "Same as the traditional Logistic Regression, we use **One vs All** method\n",
    "- The hypothesis output $h_{\\theta}(x)$ is a vector instead of a number, and the class with the largest probability will be marked with 1\n",
    "\n",
    "![W4-NN-MM](Plots/W4-NN-MM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Backpropagation\n",
    "#### Cost Function\n",
    "#### Notation\n",
    "- $L$: the total number of layers in the network\n",
    "- $s_l$: the number of units (excluding the bias unit) in layer $l$\n",
    "- $K$: the number of output units/classes\n",
    "\n",
    "##### Cost of Weight Matrix\n",
    "![W5-COST](Plots/W5-COST.png)\n",
    "- Extra summation than the Regularized Logistic Regression to account for the multiple output nodes\n",
    "    - Loop through the number of output nodes (For example, 1 to 10)\n",
    "- $\\Theta_{j,i}^{(l)}$: the weight matrix that maps from the node $i$ in the layer $l$ to the node $j$ in the next layer \n",
    "\n",
    "#### Backpropagation Algorithm - Minimize Cost Function $J(\\Theta)$\n",
    "##### Reference\n",
    "- [Backpropagation in CS231](http://cs231n.github.io/optimization-2/)\n",
    "- [J.G. Makin in UC Berkeley](https://inst.eecs.berkeley.edu/~cs182/sp06/notes/backprop.pdf)\n",
    "![W4-NN-BP](Plots/W4-NN-BP.png)\n",
    "\n",
    "- **Accumulate Losses**: Perform forward propagation to find $a^{(L)}$ (or $h_{\\theta}(x)$) of one $x^{(i)}$, and compute the loss with the corresponding $y^{(i)}$\n",
    "    - One pair at a time\n",
    "- **$\\delta^{(l)}_j = \\frac{\\partial}{\\partial z^{(l)}_j} cost(i)$: \"Error\" of cost for $a^{(l)}_j$**\n",
    "    - On the last layer, the $cost^{(L)}_j = 1/2*[a^{(L)}_j - y_j]^2$, so $\\delta^{(L)}_j = a^{(L)}_j - y_j$\n",
    "    - After find the error of the output layer $\\delta^{(L)}$, we can compute the $\\delta^{(l)}$ of the hidden layers through back-propagation\n",
    "        - The error in one node is distributed to all the nodes in the next layer that is connected to it through the weight matrix\n",
    "        - Thus, the $\\delta^{(l+1)}$ term in the formula below is not a single number, but **all the error** in the next layer\n",
    "![W5-DELTA](Plots/W5-DELTA.png)\n",
    "    - The second part of the formula comes from the g-prime\n",
    "    ![W5-GPRIME](Plots/W5-GPRIME.png)\n",
    "    - [Proof with the derivative of Sigmoid](http://mathworld.wolfram.com/SigmoidFunction.html): $\\frac{d}{dx} (\\frac{1}{1+e^{-x}}) = \\frac{e^{-x}}{(1+e^{-x})^2} = y(x)*[(1+y(x))]$\n",
    "\n",
    "- Vectorize the accumulation of $\\Delta$\n",
    "![W5-DELTA2](Plots/W5-DELTA2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "### Unrolling Parameters - Vectorization\n",
    "Both the Weight Matrix $\\Theta^{l}$ and the Gradient Matrix $\\Delta$ are in matrix form, we need to unroll/vectorize them into a long (vertical) vector\n",
    "- When using Forward Prop/BackProp, matrix will help with the computation\n",
    "- When using advanced optimization method (like `fminunc()`), it requires vector input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% Unrolling\n",
    "thetaVector = [ Theta1(:); Theta2(:); Theta3(:);];\n",
    "deltaVector = [ D1(:); D2(:); D3(:)];\n",
    "\n",
    "% Reshape back to the original form\n",
    "Theta1 = reshape(thetaVector(1:110),10,11)\n",
    "Theta2 = reshape(thetaVector(111:220),10,11)\n",
    "Theta3 = reshape(thetaVector(221:231),1,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Checking\n",
    "**Assure that the backpropagation works correctly**\n",
    "- Once we compute our gradApprox vector, we can check that gradApprox ≈ deltaVector\n",
    "- This is very **computationally expensive**, so DO NOT use it in training\n",
    "\n",
    "Based on the two-way gradient formula\n",
    "![W5-GRAD-MATH](Plots/W5-GRAD-MATH.png)\n",
    "\n",
    "Generalize to the $\\Theta$ matrix:\n",
    "![W5-GR-CHECK](Plots/W5-GR-CHECK.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% Matlab Code Example\n",
    "epsilon = 1e-4;\n",
    "for i = 1:n,\n",
    "  thetaPlus = theta;\n",
    "  thetaPlus(i) += epsilon;\n",
    "  thetaMinus = theta;\n",
    "  thetaMinus(i) -= epsilon;\n",
    "  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initialization\n",
    "Initialize all weight matrix $\\Theta$ to the same value like 0 **does not work well with Neural Network**. \n",
    "- When we backpropagate, all nodes will update to the same value repeatedly\n",
    "\n",
    "**We need to initialize weights randomly and avoid symmetry**\n",
    "![W5-RA-INIT](Plots/W5-RA-INIT.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.\n",
    "Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guideline to Build up a Neural Network\n",
    "### Pick a network architecture (Connectivity Pattern)\n",
    "- Number of input units = dimension of features x(i)\n",
    "- Number of output units = number of classes\n",
    "- Number of hidden units per layer = usually more the better\n",
    "    - Must balance with cost of computation as it increases with more hidden units\n",
    "    - Defaults: 1 hidden layer. \n",
    "    - If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer\n",
    "\n",
    "### Training a Neural Network\n",
    "1. Randomly initialize the weights\n",
    "2. Implement forward propagation to get hΘ(x(i)) for any x(i)\n",
    "3. Implement the cost function\n",
    "4. Implement backpropagation to compute partial derivatives\n",
    "5. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.\n",
    "6. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
