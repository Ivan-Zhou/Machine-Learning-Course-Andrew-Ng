{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Representation\n",
    "### Components of Neural Network\n",
    "#### A Single Logistic Neuron Unit\n",
    "- Input features\n",
    "    - The $x_0$ input is used as \"bias unit\" and always equal to 1\n",
    "- Input Wire\n",
    "- Neuron\n",
    "- Output Wire\n",
    "- Output Hypothesis $h_{\\theta}(x) = \\frac{1}{1+e^{-\\theta^Tx}}$\n",
    "\n",
    "![W4-LOGISTIC-UNIT](Plots/W4-LOGISTIC-UNIT.png)\n",
    "\n",
    "#### Activation Units\n",
    "Nodes in the intermediate/hidden layers\n",
    "- $a^{(j)}_i$: Activation of unit $i$ in layer $j$ (be aware of the notation)\n",
    "- $\\Theta^{(j)}$: matrix of weights controlling function mapping from layer $j$ to layer $j+1$\n",
    "    - Dimension of $\\Theta^{(j)}$: $s_{j+1}*(S_{j}+1)$ if the layer $j$ has $s_j$ units and the layer $j+1$ has $s_{j+1}$ units\n",
    "    - Each layer has an **additional bias unit** $x_0$/$a_0$\n",
    "\n",
    "### Forward Propagation - Compute the Hypothesis\n",
    "In the Neural Net below, we have 3 nodes in the input layers and 3 nodes in the second layer (hidden layer), so the DImension of $\\Theta^{(1)}$ is $3*4 = 12$. \n",
    "\n",
    "The computation of the second layer (hidden layer) is shown with $a^{(2)}_1,a^{(2)}_2,a^{(2)}_3$.\n",
    "\n",
    "The final output $h_{\\Theta}$ is computed with $a^{(3)}_1$\n",
    "\n",
    "![W4-FORWARD-PROP](Plots/W4-FORWARD-PROP.png)\n",
    "- On the right size, it shows the vectorized computation. Note: \n",
    "    - Remember to add bias unit (1) to each layer\n",
    "    - Use $z^{(j)}_k$ to represent the product inside the $g()$ function\n",
    "        - $z^{(j)} = \\Theta^{(j-1)}a^{(j-1)}$\n",
    "    - The $g()$ in the screenshot stands for the sigmoid function $g(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "### Examples - From AND/OR to XNOR Operation\n",
    "XNOR: gives 1 when $x_1=x_2$\n",
    "\n",
    "An important reference is the curve of the Sigmoid Function\n",
    "![W4-SIGMOID](Plots/W4-SIGMOID.png)\n",
    "![W4-NN-OPT](Plots/W4-NN-OPT.png)\n",
    "\n",
    "### Multiclass Classification\n",
    "Same as the traditional Logistic Regression, we use **One vs All** method\n",
    "- The hypothesis output $h_{\\theta}(x)$ is a vector instead of a number, and the class with the largest probability will be marked with 1\n",
    "\n",
    "![W4-NN-MM](Plots/W4-NN-MM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
