{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "## Data Compression\n",
    "- Speed up the learning algorithm\n",
    "- Reduce memory needed to store data\n",
    "- Lose the interpretability\n",
    "\n",
    "![W8-DATA-COMP1](Plots/W8-DATA-COMP1.png)\n",
    "![W8-DATA-COMP2](Plots/W8-DATA-COMP2.png)\n",
    "\n",
    "## Principal Component Analysis\n",
    "The Goal: Reduce the Dimension (number of attributes) in the dataset (from $R^n$ to $R^k$, $n > k$)\n",
    "- Find a direction (a vector of dimension $k$) onto which to project the data so as to **minimize the projection error**\n",
    "    - Project Error is the distance between the data points with direction\n",
    "    \n",
    "PCA is not linear regression\n",
    "- Linear Regression: minimize the square value of the **vertical distance** between the points and the line\n",
    "- PCA: minimize the orthorgonal distance between the points and the line\n",
    "    - And there is no y to be predicted\n",
    "\n",
    "### Implementation\n",
    "Preprocessing:\n",
    "- Mean Normalization/Feature Scaling\n",
    "\n",
    "Compute the new directions: reduce $R^n$ to $R^k$ \n",
    "- Compute \"covariance matrix\": $\\Sigma = \\frac{1}{m} \\sum_{i=1}^n(x^{(i)})(x^{(i)})^T$ ($n*n$ matrix)\n",
    "- Compute the \"eigenvectors\" of matrix $\\Sigma$ (Also a $n*n$ matrix)\n",
    "    - Use **Singular Value Decomposition**: `[U,S,V] = svd(Sigma)`\n",
    "- If we want $k$ dimensions, we select the first k columns in U matrix ($n*k$ matrix)\n",
    "\n",
    "Project the original data to the new dimensions $Z \\in R^k$\n",
    "- $X_{PCA} = U_{reduced}'*X$\n",
    "- X is given in n*m\n",
    "    - Each column represent a sample and each row is a dimension\n",
    "- Output: a $k*m$ matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sigma = 1/m*X'*X;\n",
    "[U,S,V] = svd(Sigma);\n",
    "Ureduce = U(:,1:k);\n",
    "z = Ureduce'*x;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction from compressed representation\n",
    "Go back from the compressed $z \\in R^k$ back to $x \\in R^n$\n",
    "\n",
    "We will lose variance and the loss is measured with $\\frac{\\sum_{i=1}^k S_{ii}}{\\sum_{i=1}^n S_{ii}}$\n",
    "- S comes from the Singular Value Decomposition of the covariance matrix\n",
    "- S is a diagonal and square matrix, the element along the diagonal line is $s_{ii}$\n",
    "\n",
    "### Choose Number of Principal Components ($k$)\n",
    "Criteria: keep as much variance of the data as possible\n",
    "\n",
    "#### Method 1: % Variance Retained\n",
    "Two Terms\n",
    "- Average Squared Projection Error: $\\frac{1}{m} \\sum_{i=1}^m ||x^{(i)} - x^{(i)}_{approx}||^2$\n",
    "    - $x^{(i)}_{approx}$ comes from the reconstruction from the compressed representation\n",
    "- Total Variation in the data: $\\frac{1}{m} \\sum_{i=1}^m ||x^{(i)}||^2$\n",
    "\n",
    "Set the threshold of **retaining 99% of variance**:\n",
    "- Average Squared Projection Error/Total Variation $\\leq (1-99\\%)$, or:\n",
    "- $\\frac{\\frac{1}{m} \\sum_{i=1}^m ||x^{(i)} - x^{(i)}_{approx}||^2}{\\frac{1}{m} \\sum_{i=1}^m ||x^{(i)}||^2} \\leq 0.01$\n",
    "\n",
    "We will test different k until finding the **smallest k that meets the threshold**\n",
    "\n",
    "#### Method 2: Loss of Variance\n",
    "Same logic as the Method 1, but we use $S$\n",
    "- $1-\\frac{\\sum_{i=1}^k S_{ii}}{\\sum_{i=1}^n S_{ii}} \\leq 0.01$\n",
    "- S comes from the Singular Value Decomposition of the covariance matrix: `[U,S,V] = svd(Sigma)`\n",
    "- S is a diagonal and square matrix, the element along the diagonal line is $s_{ii}$\n",
    "\n",
    "### Advices\n",
    "PCA is not suitable to avoid overfitting\n",
    "- The implementation of PCA has no consideration for y\n",
    "- Regularization is more suitable method\n",
    "\n",
    "PCA should not be used for granted\n",
    "- Before implementing PCA, first try running whatever you want to do with the original data. Only if that doesn't do what you want, then implement PCA and consider using $z^{(i)}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
