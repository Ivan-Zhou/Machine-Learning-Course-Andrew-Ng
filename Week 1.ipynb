{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defination\n",
    "> **Arthur Samuel**: \"the field of study that gives computers the ability to learn without being explicitly programmed.\"\n",
    "\n",
    "> **Tom Mitchell**: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\"\n",
    "\n",
    "### Supervised Learning\n",
    "- Given a data set in which the \"right answers\" were given\n",
    "    - Example: \n",
    "\t\t- Several actual prices for houses of difference size\n",
    "\t\t- Email spam problem\n",
    "\t\t- Classify patients into groups of having diabetes or not\n",
    "\t- Goal: produce more right answers\n",
    "- Regression problem: predict continuous valued output\n",
    "- Classification Question: predict discrete output (0/1, 0/1/2/3)\n",
    "\t- There could be more than one input (feature)\n",
    "\t\t- In the case of Probability of Tumor: Tumor size, age, clump thickness, uniformity of cell shape, etc.\n",
    "\n",
    "### Unsupervised Learning\n",
    "- Give dataset without clustering or explaining the data, and ask the machine to automatically find structure in the data and cluster individuals into types\n",
    "- Example\n",
    "\t- Given a set of news articles found on the web, group them into set of articles about the same story\n",
    "\t- Given a database of customer data, automatically discover market segments and group customers into different market segments\n",
    "\t- Cocktail Party Problem: in a party with many people and multiple microphone, analyze and identify different sources of recording to separate them\n",
    "    \n",
    "> [Course Wiki](https://share.coursera.org/wiki/index.php/ML:Main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with One Variable (Univariate)\n",
    "### Model Representation\n",
    "- Notation\n",
    "\t- $m$ = Number of training examples\n",
    "\t- $x$ = input variable (feature)\n",
    "\t- $y$ = output variable (target)\n",
    "\t- $(x, y)$ = one training example\n",
    "\t- $(x^i, y^i)$ = $i^{th}$ training example \n",
    "\t- $h$ = hypothesis\n",
    "\t- $\\theta$ = Parameters\n",
    "- Use the training set and the learning algorithm to learn a hypothesis function $h(x)$\n",
    "    - We use the hypothesis to map x's to y's $h: x -> y$\n",
    "    - Linear Example: $h_{\\theta}(x) = \\theta_0+\\theta_1 x$\n",
    "\n",
    "### Cost Function\n",
    "- Hypothesis Function: $h_θ(x) = θ_0 + θ_1*x$\n",
    "- Cost Function: $J(\\theta_0,\\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m (\\hat{y_i} - y_i)^2=\\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x_i)-y_i)^2$\n",
    "    - Measure the (vertical) distance between the predicted value and the actual value\n",
    "    - $\\frac{1}{2}$ is used as a convenience for the computation of the graident descent\n",
    "- **Goal**: Minimize $J(\\theta_0,\\theta_1)$\n",
    "    - Choose $θ_0$, $θ_1$ so that $h_θ(x)$ is close to y for the training examples\n",
    "    - Find the best fit straight line for the data\n",
    "    \n",
    "#### Visualization of Cost Function in a 3D Plot\n",
    "![3D-Cost-Function](Plots/W1-CF-3D.png)\n",
    "\n",
    "#### Visualization of Cost Function in a Contour Plot\n",
    "A contour plot is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line. \n",
    "![Contour-Cost-Function](Plots/W1-CF-CTP.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Minimize the cost function $J(\\theta_0,\\theta_1)$ with the Gradient Descent\n",
    "\n",
    "### Gradient Descent Algorithm\n",
    "repeat until convergence{\n",
    "\n",
    "  $\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta_0,\\theta_1), \\forall j \\in J$\n",
    "\n",
    "}\n",
    "\n",
    "- **Simultaneous update**: all $\\theta$'s are updated with the same pair of the $\\theta$ values on the rhs\n",
    "    - Do not use the updated $\\theta$ to compute others until all $\\theta$'s have been updated\n",
    "- Learning Rate $\\alpha$: \n",
    "    - If $\\alpha$ is too small, gradient descent can be slow\n",
    "    - If $\\alpha$ is too large, gradient descent can overshoot the minimum or even diverge\n",
    "\n",
    "#### Gradient Descent in Linear Regression\n",
    "All linear models are **convex function** - they only have one unique local minimum\n",
    "    - Gradient Descent is guaranteed to converge to the global minimum\n",
    "    - Bowl-shaped function\n",
    "    \n",
    "![Gradient Descent in Linear Regression](Plots/W1-GD.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
